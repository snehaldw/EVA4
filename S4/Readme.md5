Model Summary:

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 26, 26]             160
       BatchNorm2d-2           [-1, 16, 26, 26]              32
            Conv2d-3           [-1, 16, 24, 24]           2,320
       BatchNorm2d-4           [-1, 16, 24, 24]              32
            Conv2d-5           [-1, 16, 22, 22]           2,320
       BatchNorm2d-6           [-1, 16, 22, 22]              32
         MaxPool2d-7           [-1, 16, 11, 11]               0
            Conv2d-8            [-1, 8, 11, 11]             136
       BatchNorm2d-9            [-1, 8, 11, 11]              16
           Conv2d-10             [-1, 16, 9, 9]           1,168
      BatchNorm2d-11             [-1, 16, 9, 9]              32
        Dropout2d-12             [-1, 16, 9, 9]               0
           Conv2d-13             [-1, 16, 7, 7]           2,320
      BatchNorm2d-14             [-1, 16, 7, 7]              32
        Dropout2d-15             [-1, 16, 7, 7]               0
           Conv2d-16             [-1, 16, 5, 5]           2,320
      BatchNorm2d-17             [-1, 16, 5, 5]              32
        Dropout2d-18             [-1, 16, 5, 5]               0
           Conv2d-19             [-1, 10, 1, 1]           4,010
================================================================
Total params: 14,962
Trainable params: 14,962
Non-trainable params: 0



Execution details:

  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
loss=0.08122958242893219 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.98it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0511, Accuracy: 9829/10000 (98.29%)

loss=0.0453539676964283 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.14it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0483, Accuracy: 9848/10000 (98.48%)

loss=0.12728621065616608 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.28it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0323, Accuracy: 9886/10000 (98.86%)

loss=0.08364065736532211 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.88it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0352, Accuracy: 9879/10000 (98.79%)

loss=0.06700920313596725 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.16it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0357, Accuracy: 9877/10000 (98.77%)

loss=0.05645535886287689 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.20it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0280, Accuracy: 9907/10000 (99.07%)

loss=0.11668780446052551 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 39.78it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0272, Accuracy: 9900/10000 (99.00%)

loss=0.052255887538194656 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.59it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0258, Accuracy: 9907/10000 (99.07%)

loss=0.017524057999253273 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.60it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0292, Accuracy: 9894/10000 (98.94%)

loss=0.02971825562417507 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.25it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0283, Accuracy: 9906/10000 (99.06%)

loss=0.02627432532608509 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.84it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0251, Accuracy: 9912/10000 (99.12%)

loss=0.03161584958434105 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.18it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0253, Accuracy: 9914/10000 (99.14%)

loss=0.0724056139588356 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.68it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0234, Accuracy: 9917/10000 (99.17%)

loss=0.01306260097771883 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.49it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0240, Accuracy: 9922/10000 (99.22%)

loss=0.01360087376087904 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.52it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0228, Accuracy: 9925/10000 (99.25%)

loss=0.008152698166668415 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.75it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0242, Accuracy: 9923/10000 (99.23%)

loss=0.004348486661911011 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.32it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0255, Accuracy: 9917/10000 (99.17%)

loss=0.07989465445280075 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.71it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0242, Accuracy: 9926/10000 (99.26%)

loss=0.02049439586699009 batch_id=468: 100%|██████████| 469/469 [00:11<00:00, 40.26it/s]

Test set: Average loss: 0.0236, Accuracy: 9923/10000 (99.23%)

